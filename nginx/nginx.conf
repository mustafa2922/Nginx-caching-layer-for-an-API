# How many worker processes nginx spawns.
# 'auto' = one per CPU core. Each worker handles thousands of connections.
worker_processes auto;

events {
  # Max simultaneous connections per worker process
  worker_connections 1024;
}

http {

  # ─── CACHE ZONE DEFINITION ───────────────────────────────────────────────
  # This MUST be in the http block, not inside server or location.
  # You define the storage here once, then reference it by name below.
  #
  # /var/cache/nginx  → where on disk to store cached files
  # levels=1:2        → creates subdirectories (prevents too many files in one dir)
  # keys_zone=api_cache:10m → name this cache "api_cache", use 10MB RAM for the index
  # max_size=500m     → max 500MB of cached responses on disk
  # inactive=10m      → evict entries not accessed in 10 minutes
  # use_temp_path=off → write directly to cache dir (avoids extra copy, faster)
  proxy_cache_path /var/cache/nginx
    levels=1:2
    keys_zone=api_cache:10m
    max_size=500m
    inactive=10m
    use_temp_path=off;

  server {
    listen 80;
    server_name localhost;

    # ─── LOCATION /api/ ───────────────────────────────────────────────────
    # Anything starting with /api/ gets proxied to Express
    location /api/ {

      # Strip /api/ prefix and forward to Express on port 3000
      # e.g. GET /api/movies → http://backend:3000/movies
      proxy_pass http://backend:3000/;

      # Use HTTP/1.1 for proxying (required for keep-alive connections)
      proxy_http_version 1.1;

      # Forward the real client info to Express
      proxy_set_header Host $host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

      # ─── CACHE DIRECTIVES ───────────────────────────────────────────────

      # Enable caching using our defined zone
      proxy_cache api_cache;

      # What makes a cached entry unique — by default it's the full URL
      # We include the method so POST /movies and GET /movies are different keys
      proxy_cache_key "$request_method$host$request_uri";

      # Cache 200 responses for 2 minutes, 404s for 30 seconds
      proxy_cache_valid 200 2m;
      proxy_cache_valid 404 30s;

      # If the client sends Cache-Control: no-cache, bypass the cache
      # This lets you force a fresh response during testing
      proxy_cache_bypass $http_cache_control;

      # Don't cache POST, PUT, DELETE — only cache GET and HEAD
      # $request_method will be "POST", "PUT" etc. — truthy = no cache
      proxy_no_cache $request_method;

      # If Express is down, serve stale cache instead of a 502 error
      proxy_cache_use_stale error timeout http_500 http_502 http_503 http_504;

      # Add a response header so we can see in browser devtools whether it was
      # a HIT, MISS, BYPASS, or EXPIRED
      add_header X-Cache-Status $upstream_cache_status;

      # Add a header showing which nginx worker served the request (debugging)
      add_header X-Nginx-Worker $pid;
    }

    # ─── LOCATION /health ─────────────────────────────────────────────────
    # Proxy the health check through without caching
    location /health {
      proxy_pass http://backend:3000/health;
    }
  }
}